# This default config file shows all arguments.
# NOTE: If your custom config file doesn't have all of following arguments (not including the advanced FL methods'),
# the defaults can be found in src/utils/constants.py

# The method name must be identical to the file name in src/server/<method>.py
method: fedbeat

dataset:
  # [mnist, cifar10, cifar100, emnist, fmnist, femnist, medmnist,
  # medmnistA, medmnistC, covid19, celeba, synthetic, svhn,
  # tiny_imagenet, cinic10, domain]
  name: mnist

model:
  name: lenet5

  # Whether to use torchvision integrated model weights.
  # Has no effect if model is lenet5, 2nn or fedavgcnn
  use_torchvision_pretrained_weights: true

  # The model parameters `.pt` file relative path to the directory of FL-bench.
  # This feature is enabled only when `all_model_params_personalized=False`,
  # which is defined and fixed by each FL method.
  external_model_weights_path: null

# The learning rate scheduler that used for client local training.
# Can be null if no lr_scheduler is needed.
lr_scheduler:
  name: null # [null, step, cosine, constant, plateau]
  step_size: 10 # step
  gamma: 0.1 # [step, plateau]
  T_max: 10 # cosine
  eta_min: 0 # cosine
  factor: 0.3334 # [constant, plateau]
  total_iters: 5 # constant
  mode: min # plataeu
  patience: 10 # plateau
  threshold: 1.0e-4 # plateau
  threshold_mode: rel # plateau
  cooldown: 0 # plateau
  min_lr: 0 # plateau
  eps: 1.0e-8 # plateau
  last_epoch: -1

# The optimizer that used for client local training.
optimizer:
  name: sgd # [sgd, adam, adamw, rmsprop, adagrad]
  lr: 0.1
  dampening: 0 # for SGD
  weight_decay: 0
  momentum: 0 # for [SGD, RMSprop]
  alpha: 0.99 # for RMSprop
  nesterov: false # for SGD
  betas: [0.9, 0.999] # for [Adam, AdamW]
  amsgrad: false # for [Adam, AdamW]

mode: serial # [serial, parallel]
# It's fine to keep these configs. if mode is 'serial', these configs will be ignored.
parallel:
  # The IP address of the selected ray cluster.
  # Default as null, which means if there is no existing ray cluster,
  # then Ray will create a new cluster at the beginning of the experiment
  # and destroy it at the end.
  # More details can be found in https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html.
  ray_cluster_addr: null # [null, auto, local]

  # The amount of computational resources you allocate for your Ray cluster.
  # Default as null for all.
  num_cpus: null
  num_gpus: null

  # Should be set larger than 1, or training mode fallback to `serial`
  # Set a larger `num_workers` can further boost efficiency,
  # but also increases the computational overhead.
  num_workers: 2

common:
  seed: 42 # Random seed of the run.
  join_ratio: 0.2 # Ratio for (client each round) / (client num in total).
  global_epoch: 80 # Number of global epochs, also called communication round.
  local_epoch: 5 # Number if epochs of client local training.
  batch_size: 32 # Data batch size for client local training.
  reset_optimizer_on_global_epoch: true # Whether to reset optimizer on each global epoch.

  # The ratio of stragglers (set in [0, 1]).
  # Stragglers would not perform full-epoch local training as normal clients.
  # Their local epoch would be randomly selected from range [straggler_min_local_epoch, local_epoch).
  straggler_ratio: 0
  straggler_min_local_epoch: 0

  # How to deal with parameter buffers (in model.buffers()) of each client model.
  # global (default): buffers will be aggregated like other model parameters.
  # local: clients' buffers are isolated.
  # drop: clients will drop their buffers after training done.
  buffers: global # [local, global, drop]

  # Whether to evaluate client local models (that before and after local training) on client side.
  # You can deactivating this for acclerating training.
  # NOTE: deactivate this feature will affect features like logging and monitoring.
  client_side_evaluation: true

  # The evaluation settings for client side and server side.
  test:
    # For example, Set client.test as true to evaluate on local testsets from selected clients with client local models.
    # Frequency is set by `client.interval`. Negative value for disabling.
    client:
      interval: 100
      finetune_epoch: 0 # Number of epochs for clients fine-tunning their models before testing.
      train: false
      val: false
      test: true
    # For example, set server.test as true to evaluate on a centralized testset (created by aggregating all clients' local testsets)
    # with the updated global model at the end of a communication round.
    # Frequency is set by `server.interval`. Negative value for disabling.
    # NOTE: If clients have personalized model parameters like local buffers or classifiers, centralized evaluation for the global model is disabled.
    server:
      interval: 1
      train: false
      val: false
      test: true
      # Whether to evaluate the global model in train mode or eval mode.
      # Evaluating in train mode allows to get better batchnorm statistics, but is dependent on the order of the data.
      model_in_train_mode: false

  verbose_gap: 10 # Interval round of displaying clients training performance on terminal.
  monitor: null # [null, visdom, tensorboard]
  use_cuda: true # Whether to use cuda for training.

  save_log: true # Whether to save log files in out/<method>/<start_time>.
  save_model: false # Whether to save model weights (*.pt) in out/<method>/<start_time>.
  save_learning_curve_plot: true # Whether to save learning curve figure (*.png) in out/<method>/<start_time>.
  save_metrics: true # Whether to save metrics (*.csv) in out/<method>/<start_time>.

  # Whether to delete output files after user press `Ctrl + C`,
  # which indicates that the run is removable.
  delete_useless_run: true

# You can set specific arguments for advanced FL methods also.
# FL-bench uses FL method arguments by args.<method>.<arg>.
# You need to follow the key set in `get_hyperparams()` in class <method>Server, src/server/<method>.py
# FL-bench will ignore these arguments if they are not supported by the selected method,
# e.g., if you are running FedProx, then pFedSim arguments will be ignored.
fedprox:
  mu: 0.01
pfedsim:
  warmup_round: 0.5
fedbeat:
  upload:
    upload_backbone: "state_dict"
    upload_logits: false
    upload_posterior: false

  backbone:
    freeze_during_posterior: true   # ← 保留这个，后插阶段冻结主干

  posterior:
    enabled: true
    where: "after_logits"
    type: "mlp"
    hidden_dim: 512
    depth: 2
    ensemble_N: 8
    temperature: 1.0

  loss:
    ce_weight: 1.0
    reweight:
      enabled: true
      mode: "boundary"
      normalize: true
      clip_min: 0.0
      clip_max: 1.0
    kl:
      enabled: true
      beta: 2.0
      target: "backbone_logits"
      apply_to: "adv"

  rs:
    enabled: false  # <- 打开 RS baseline
    sigma: 0.35
    n_samples: 16    # 评测取 4~16 之间比较常见；越大越慢

  denoise:
    enabled: false     # <- 打开 Denoise baseline
    kind: "median"
    median_kernel: 5
    tv_weight: 0.1
    ae_ckpt: ""

  gap:
    enabled: false        # 打开后启用“生成对抗净化”推理防御
    kind: "tiny_unet"     # 目前提供 tiny_unet
    ckpt: ""              # 生成器权重路径（留空则用随机初始化 -> 仅调通，不建议正式评测）
    noise_sigma: 0.0      # 净化时前加微噪（可略增强）
    tta_n: 1              # 多次净化并平均（>1 提升稳健，略增耗时）

  eval:
    # === NEW: Adv 评估钩子（写入 metrics.csv 的 adv_acc_* 与 robust_gap_* 列）
    adv_enabled: true           # 置 false 完全关闭黑盒评估（训练更快）
    interval_rounds: 1          # 每多少轮评一次（建议 mnist=1, cifar10=5）
    max_eval_examples: 256     # 每次最多用多少张做 adv 评估，避免太慢

    # （可选）客户端本地鲁棒性“均值”评测
    client_adv_enabled: false      # 需要就开；默认关以省时
    client_adv_max_clients: 10     # 每轮最多取多少个已选客户端参与

    # 原有
    save_adv_examples: false
    report: [ "clean_acc", "robust_acc", "query_budget", "success_rate" ]
    attacks_to_eval: [active]  # ["active"] 或 ["square","spsa",...]
    eval_step_override: # 只作用于评估，不影响训练
      square: 2000
      spsa: 1500
      onepixel: 50
      pixle: 60

  # 两阶段控制：A 阶段直接用 common.local_epoch；B 阶段单独控制
  epochs_beat: 0

  attacks:
    active: "square"
    eps: 0.3
    steps: 600
    budget: 10000
    list:
      - name: "square"
        p_init: 0.8
        queries: 600
        loss: "margin"
        resc_schedule: true
        seed: 42
      - name: "spsa"
        nb_iter: 5000
        delta: 0.01
        lr: 0.01
        nb_sample: 128
        max_batch_size: 64
      - name: "onepixel"
        steps: 75
        pixels: 1
        popsize: 10
        inf_batch: 128
      - name: "pixle"
        max_iterations: 75
        x_dimensions: [2,10]
        y_dimensions: [2,10]
        pixel_mapping: "random"
        restarts: 5
        update_each_iteration: false

  # === NEW: 轻量提速选项（默认温和，不牺牲太多鲁棒性） ===
  speed:
    beat_stride: 32         # 每 32 个 batch 才做一次 BEAT 处理（1 表示每个 batch 都做）
                           # 注意：若此参数减小，会大幅增加运行时间
    beat_client_prob: 1.0  # 每轮执行 BEAT 的客户端概率（1.0=全部；0.5≈一半客户端）
    amp: true              # 对 BEAT 头的前后向启用 AMP（需 CUDA）
  # === /NEW ===